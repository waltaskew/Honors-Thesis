QUICKSTART FOR IR00:
export LD_LIBRARY_PATH=/aut/proj/ir/wsaskew/System/local/lib:$LD_LIBRARY_PATH
export LD_RUN_PATH=/aut/proj/ir/wsaskew/System/local/lib:$LD_RUN_PATH
export MANPATH=/aut/proj/ir/wsaskew/System/local/man:$MANPATH
export PATH=/aut/proj/ir/wsaskew/System/local/bin:$PATH
export PYTHONPATH=/aut/proj/ir/wsaskew/System/pythonpath:$PYTHONPATH

look at example.py and example1.py for examples

REQUIREMENTS:

python2.6:
http://www.python.org/download/

for parsing html and xml:
libxml, libxslt and the lxml python bindings for these libraries

libxml:
http://xmlsoft.org/downloads.html
libxslt:
http://xmlsoft.org/XSLT/downloads.html
lxml:
http://pypi.python.org/pypi/lxml/

for working with ARFF files:
antlr and the arff package

antlr:
http://www.antlr.org/download/Python/
arff package:
http://www.mit.edu/~sav/arff/dist/

INSTALLATION:
copy the code directory into someplace on your pythonpath
example:
cp -R cooccurrence_similairty ~/python_code/
touch ~/python_code/__init__.py
export PYTHONPATH=~/python_code:$PYTHONPATH

EXAMPLE USAGE:

NOTE:
All arguments in the form arg=Value are optional arguments.
If they are not supplied, reasonable default will be used.

First, an experimenter object needs to be created.
The constructor takes one argument, the name of the directory
in which experiment data will be saved.  'experiment_dir' should 
either be an empty directory, a path of a non-existant file or directory,
or a path to a prieviously created experiment directory.
This directory will hold all of the results from experiments,
and retains consistency across multiple runs:

e = Experimenter('experiment_dir')

Next, one or more corpora should be indexed for experimentation:
A file containing stop words ('stop_file') is optional, but recomended.
sync_freq controls have often the indexed will be synchronized to disk
and wiped from memory.  Synchronization will occur after synch_freq
docuemnts are processed.  High values yield faster indexing and higher
memory usage, and vice versa.

e.add_to_index(corpus_dir, 'phpBB',
    stop_file=/home/stop_words.txt, synch_freq=10000)

When indexing an XML corpus, a list of important tags must be sepcified.

e.add_to_index(corpus_dir, 'xml',
    tag_file='~/tag_file')

a tag file should be in the following format:

TitleTag: ArticleTitle
DelimiatorTag: MedlineCitation
HeadingTag: MeshHeading
AbstractText

TitleTag specifies where the title of a document may be found
HeadingTag specifies the location of heading or meta-information
DelimiatorTag specifies a tag which separates documents from each other,
if a single xml file holds multiple documents
Tags which are not preceded by a label (such as AbstractText in the
above example) specify the location of text to be parsed out.
An arbitrary number of such tags may be specified.

Now experiments can be performed:

e.perform_experiment(targets, synonym_file=None,
    window=50, pmi_threshold=0,
    relation_threshold=0, truth_DB=truth,
    truth_function='2_way_mild')

Only nescessary intermediate steps are performed.  For example,

e.perform_experiment(targets, synonym_file=None,
    window=50, pmi_threshold=0,
    relation_threshold=0, truth_DB=truth,
    truth_function='2_way_mild')

e.perform_experiment(targets, synonym_file=None,
    window=50, pmi_threshold=0,
    relation_threshold=1000, truth_DB=truth,
    truth_function='2_way_mild')

will reuse the results from calculating cooccurrences with a
window of 50 and a pmi threshold of 0, only recalculating
relations using a relation threshold of 1000

If the truth_DB or truth_function are not provided, then
feature files will be created without truth values.

experiment variables have the following defaults:
cooccurrence_window = 25
pmi_thresholld = 0
relation_threshold = 0

A synonym file can optionally be provided.  A synonym file
should be of the format:
synonym:target
and will cause all occurrecnes of synonym to be counted as
occurrences of target.

Note that one of the features of the generated arff files
is the names of the diseases in the instance.
This is (probably) not a feature that should be used
for classification.  With WEKA, use the following command
to strip out all string fields when used for training
and classification:

java weka.classifiers.meta.FilteredClassifier
  -F weka.filters.unsupervised.attribute.RemoveType
  -W weka.classifiers.trees.J48
  -t train.arff -T test.arff -p 5

The option:  -F weka.filters.unsupervised.attribute.RemoveType
removes all string fields, and the only string field in the
arff file is the disease names.  If you actually decide to
use string fields in the arff file, you will need to use a
more clever filter. 

You can generate documentation for any of the python modules 
with the pydoc command.

send bugs or complaints to: waltaskew@gmail.com

SEE ALSO:
example.py
